\chapter{Imagefit analysis routine} \label{app:imagefitManual}
"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."

General procedure, should probably suggest moving to Github and warn about software migration breaking older versions of code.

Currently everything is working in 2015b? Can it work in a newer version?

\section{Background removal}

Covers \emph{imagefit\_Background\_PCA}

Would like to remove noisy fringes to fit more easily

\subsection{Principal component analysis}

be sure to discuss tradeoffs such as matrix size, basis size, and number of images

The Anderson and Cornell groups have adapted two statistical techniques used in astronomical data processing to the analysis of images of ultracold atom gases. Image analysis is necessary for obtaining quantitative information about the behavior of an ultracold gas under different experimental conditions. Until now, the preferred method has been to find a shape (such as a Gaussian) that looks like the results and write an image-fitting routine to probe a series of photographs. The drawback is that information extracted this way will be biased by the model chosen.

The two groups recently employed model-free analysis techniques to extract results from interferometry experiments on Bose-Einstein condensates (BECs). The statistical processing techniques were able to rapidly pinpoint correlations in large image sets, helping the researchers uncover unbiased experimental results. Using the techniques, graduate student Steve Segal, former graduate student Quentin Diot, Fellows Eric Cornell and Dana Anderson, and a colleague from Worcester Polytechnic Institute calibrated their interferometer, identified and mitigated some noise sources, and unearthed signal information partially buried in the noise generated during the BEC experiment. By looking for correlations and relationships between pixels in a series of images (a), the researchers were able to clearly "see" changes in the overall number of atoms (d), changes in the vertical positions of three peaks in a momentum distribution (c), and changes in the fraction of atoms in the central peak (b), which was the primary experimental signal.

The results were obtained with principal component analysis (PCA) and independent component analysis (ICA). PCA identified simple pixel correlations and looked for areas of maximum variance. Such areas provided an idea about where to look for changes in size, structure, or position of the ultracold atom cloud. The PCA analysis was sufficient for calibrating the interferometer and debugging the experiment. It also provided an idea of size changes in one or more features of the experiment. However, the PCA analysis alone wasn’t perfect. ICA was required to extract the most important information about the experiment, i.e., the fraction of the total number of atoms in one of three clouds. Using preprocessed data from a PCA analysis, ICA was able to test whether the values of neighboring pixels were statistically independent from one another. With this information, ICA could then determine relative differences in the experimental signal and separate its individual features.

Segal thinks physicists in the ultracold atomic physics field will be intrigued by the potential of using the PCA and ICA techniques to probe their experimental images. There are only two caveats: The techniques require 10–100 images, and their application to ultracold atom-cloud experiments is still in its infancy.   - Julie Phillips

\subsection{Comparison of PCA implementations}

\section{Fitting the spatial distribution}

Covers \emph{imagefit\_NumDistFit}

\section{Evaluating fit parameters}

Covers \emph{imagefit\_ParamEval}

\subsection{Writing new plug-ins}

\subsection{Suggested improvement}

Move all data structures (or at least the experimental plugin ones) to be table based so that everything is held at the same level. This would make it much easier to be flexible with different analyses (since you're not imposing a structure that might be restrictive later)

One of the key lessons I've learned is to be flexible when starting new projects. You don't know where the data is going to take you or what may come up as an interesting/useful perspective for developing and questioning hypotheses. Tables are a scheme that I've have realized are very common in data management and analysis. The popular python package Pandas, uses datatables exclusively and was the impetus for my interest in the data structure. Tables also make it easy to export and share your data via spreadsheet applications (assuming you use simple datatypes within each cell) Lastly, this is a warning that no matter what you end up doing with the data, you will inevitably have to spend time organizing and reorganizing it at times. With less structure imposed on the data you'll be able to manipulate things more easily and (most importantly to Tom) more quickly.