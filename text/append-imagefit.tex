\chapter{Imagefit analysis routine} \label{app:imagefitManual}
General procedure, should probably suggest moving to Github and warn about software migration breaking older versions of code.

Currently everything is working in 2015b? Can it work in a newer version?

\section{Background removal}

Covers \emph{imagefit\_Background\_PCA}

Would like to remove noisy fringes to fit more easily

\subsection{Principal component analysis}

be sure to discuss tradeoffs such as matrix size, basis size, and number of images

The Anderson and Cornell groups have adapted two statistical techniques used in astronomical data processing to the analysis of images of ultracold atom gases. Image analysis is necessary for obtaining quantitative information about the behavior of an ultracold gas under different experimental conditions. Until now, the preferred method has been to find a shape (such as a Gaussian) that looks like the results and write an image-fitting routine to probe a series of photographs. The drawback is that information extracted this way will be biased by the model chosen.

The two groups recently employed model-free analysis techniques to extract results from interferometry experiments on Bose-Einstein condensates (BECs). The statistical processing techniques were able to rapidly pinpoint correlations in large image sets, helping the researchers uncover unbiased experimental results. Using the techniques, graduate student Steve Segal, former graduate student Quentin Diot, Fellows Eric Cornell and Dana Anderson, and a colleague from Worcester Polytechnic Institute calibrated their interferometer, identified and mitigated some noise sources, and unearthed signal information partially buried in the noise generated during the BEC experiment. By looking for correlations and relationships between pixels in a series of images (a), the researchers were able to clearly "see" changes in the overall number of atoms (d), changes in the vertical positions of three peaks in a momentum distribution (c), and changes in the fraction of atoms in the central peak (b), which was the primary experimental signal.

The results were obtained with principal component analysis (PCA) and independent component analysis (ICA). PCA identified simple pixel correlations and looked for areas of maximum variance. Such areas provided an idea about where to look for changes in size, structure, or position of the ultracold atom cloud. The PCA analysis was sufficient for calibrating the interferometer and debugging the experiment. It also provided an idea of size changes in one or more features of the experiment. However, the PCA analysis alone wasnâ€™t perfect. ICA was required to extract the most important information about the experiment, i.e., the fraction of the total number of atoms in one of three clouds. Using preprocessed data from a PCA analysis, ICA was able to test whether the values of neighboring pixels were statistically independent from one another. With this information, ICA could then determine relative differences in the experimental signal and separate its individual features.


\subsection{Comparison of PCA implementations}

\section{Fitting the spatial distribution}

Covers \emph{imagefit\_NumDistFit}

\section{Evaluating fit parameters}

Covers \emph{imagefit\_ParamEval}

\subsection{Writing new plug-ins}

\subsection{Suggested improvement}

The primary upgrade to this software would be an overhaul of the internal state variables.
A complete rewrite of the imagefit routine was one of the first projects undertaken during this PhD, during such a time as the meanings of precise variable names was not well known.
Unfortunately, this has led to vestigial variable names such as "imagevco\_ atom" for the primary independent variable used for scan analysis.
Additionally, while the plug-in nature of the ParamEval routine has proven to be incredibly useful for the development flexible experiment specific analysis routines, we have found the heavy reliance on structs \hl{ref} for variable passing and storage to be prohibitively restrictive.

The imagefit routine was primarily written with the streamlining of first order image analysis as the design goal.
We consider this to be the extraction of atom number and temperature versus a single scanned variable.
However, the natural extension is to look for variations across higher dimensions, or what we deem second order analysis.
Here is where structs are not useful since they tend to force a hierarchical variable relationship which complicates analysis where the variable of interest was not the explicit independent variable scanned.
With this in mind, we highly recommend any future improvements to this process focus on the usage of tables \hl{ref} for storing and passing data.
These data structures consider all data at the same level and are widely used in data science.
Finally, we note that this issue is not a new problem on the Neutral apparatus. 
But recent advancements to the experimental stability and control software have ballooned the amount of data we have been generating which is noticeably exacerbating this concern.

One of the key lessons I've learned is to be flexible when starting new projects. 
You don't know where the data is going to take you or what may come up as an interesting/useful perspective for developing and questioning hypotheses. 
Tables are a scheme that I've have realized are very common in data management and analysis. 
The popular python package Pandas, uses datatables exclusively and was the impetus for my interest in the data structure. 
Tables also make it easy to export and share your data via spreadsheet applications (assuming you use simple datatypes within each cell).
Lastly, this is a warning that no matter what you end up doing with the data, you will inevitably have to spend time organizing and reorganizing it at times. 
With less structure imposed on the data you'll be able to manipulate things more easily and (most importantly to Tom) more quickly.